{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)\n",
    "\n",
    "*by deepmind on NIPS 2016*\n",
    "- SEOHASONG - Korea Univ.\n",
    "- 2018.03.25\n",
    "- Deep Paper Study 신촌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "사람들은 목적달성을 위해 경영, 공학, 정치 등 다양한 분야에서 결과에 영향을 주는 변수를 찾고자 노력합니다.\n",
    "\n",
    "사람이 직접 변수를 찾아낼 수도 있지만, 기계학습이 발전하면서\n",
    "컴퓨터가 자동으로 변수를 찾는 방식도 주목받고 있습니다.\n",
    "\n",
    "기계학습에서 알고리즘은 최적화 알고리즘입니다.\n",
    "최적화알고리즘은 예측이 틀린 정도가 최소가 되도록 설계됩니다.\n",
    "\n",
    "컴퓨터는 데이터에 알고리즘을 적용해 자동으로 적절한 변수를 찾는 역할을 하며,\n",
    "여기 사용되는 알고리즘은 사람이 직접 만들어 줘야 합니다.\n",
    "\n",
    "이 논문에서는 사람이 직접 만들던 최적화 알고리즘을 학습하는 문제를 논의합니다.\n",
    "\n",
    "- 기존에는 사람이 만든 최적화 알고리즘을 통해 변수를 학습했습니다.\n",
    "- 앞으로는 최적화 알고리즘을 변수로 두고 그것이 무엇일지 자동으로 학습하고자 합니다.\n",
    "\n",
    "이 논문의 목적은 *특정 분류의 문제에 대해 잘 동작하는 알고리즘*을 만드는 일련의 과정을 개발하는 것입니다.\n",
    "\n",
    "*알고리즘 만들기*를 우리가 풀어야 하는 문제로 생각하는 것을 **Meta learning**이라고 부릅니다.\n",
    "\n",
    "Meta learning 관련 논문으로는 AutoML등이 있으며,\n",
    "\n",
    "Meta learning은 Few-shot learning(One-shot, Zero-shot 등등)과도 관련이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "일반적으로 기계학습 문제는 목적함수라고 불리는\n",
    "\n",
    "### $f(\\theta),\\quad\\theta\\in\\Theta$\n",
    "\n",
    "를 최적화 하는 문제입니다.\n",
    "\n",
    "식 $f(\\theta)$을 최소화(최대화) 하는 파라미터 $\\theta \\in \\Theta$를 찾는 것이 목표입니다.\n",
    "\n",
    "### $\\theta^* = arg\\ min_{\\theta\\in\\Theta}\\ f(\\theta)$\n",
    "\n",
    "식 $f$는 보통 파라미터 $\\theta$에 대해 미분 가능한 함수이며,\n",
    "\n",
    "이경우 일반적으로 단계적으로 업데이트가 이루어지는 Gradient descent 방식을 이용합니다.\n",
    "\n",
    "### $\\theta_{t+1} = \\theta_t\\ -\\ \\alpha_t\\nabla f(\\theta_t)$\n",
    "\n",
    "Gradient descent방식은 치명적인 약점이 있습니다.\n",
    "\n",
    "***단계적으로 한번 미분한 값만 고려하고 두번 미분한 값은 고려하지 못합니다.***\n",
    "\n",
    "Gradient descent가 아닌 전통적인 최적화 기법에서는 이런 문제를 피할 방법이 있습니다.\n",
    "\n",
    "함수의 두번 미분한 정도(곡률)의 정보를 이용하는 것입니다.\n",
    "\n",
    "> 참조\n",
    "> - Hessian matrix of second-order partial derivatives\n",
    "> - Generalized Gauss-Newton matrix\n",
    "> - Fisher information matrix\n",
    "> - ...\n",
    "\n",
    "현대의 많은 최적화 기법은 업데이트 규칙을 특정 문제에 깔맞춤하는 방식으로 이루어 졌습니다.\n",
    "\n",
    "예를들어, 요즘 유행하고 있는 딥러닝 분야에서는\n",
    "\n",
    "- 고차원 백터공간에서\n",
    "- Convex가 아닌 형태에서\n",
    "\n",
    "효과적인 업데이트 규칙을 이용합니다.\n",
    "\n",
    "> 참조\n",
    "> - momentum\n",
    "> - Rprop\n",
    "> - Adagrad\n",
    "> - RMSprop\n",
    "> - ADAM\n",
    "> - ...\n",
    "\n",
    "```\n",
    "More focused methods can also be applied when more structure of the optimization problem is known [Martens and Grosse, 2015]. In contrast, communities who focus on sparsity tend to favor very different approaches [Donoho, 2006, Bach et al., 2012]. This is even more the case for combinatorial optimization for which relaxations are often the norm [Nemhauser and Wolsey, 1988].\n",
    "```\n",
    "\n",
    "현실의 각 산업군에서는 해당 분야에서 잘 동작하는 최적화 방식을 연구하지만,\n",
    "\n",
    "각 방식들은 다른 분야에서는 제대로 동작하지 않을 수도 있습니다.\n",
    "\n",
    "```\n",
    "Moreover the No Free Lunch Theorems for Op- timization [Wolpert and Macready, 1997] show that in the setting of combinatorial optimization, no algorithm is able to do better than a random strategy in expectation.\n",
    "```\n",
    "\n",
    "이러한 연구들이 시사하는 바는, 일반적으로\n",
    "\n",
    "1. 분야를 세분화 하고\n",
    "2. 세분화된 분야에 특화된 방식을 찾는 것\n",
    "\n",
    "이 유일한 성능 향상의 접근법일수도 있다는 것입니다.\n",
    "\n",
    "이 논문은 업데이트 규칙을 사람이 직접 만드는 방식에서 학습되는 방식으로 다르게 구하고자 합니다.\n",
    "\n",
    "학습되는 optimizer(최적화 방식)를 여기서는 $g$라고 명명하겠습니다.\n",
    "\n",
    "우리는 $g$를 파라미터의 집합 $\\phi$에 깔맞춤 되도록 할 것입니다.\n",
    "\n",
    "기존의 gradient방식은\n",
    "\n",
    "### $\\theta_{t+1} = \\theta_t\\ -\\ \\alpha_t\\nabla f(\\theta_t)$\n",
    "\n",
    "다음과 같이 수정됩니다.\n",
    "\n",
    "### $\\theta_{t+1} = \\theta_t\\ -\\ g_t(\\nabla f(\\theta_t), \\phi)$\n",
    "\n",
    "- 여기서 $g$는 meta-learner라고 부르기도 합니다.\n",
    "- 여기서 $f$는 learner라고 부르기도 합니다.\n",
    "\n",
    "![img](img/0.png)\n",
    "\n",
    "> 왼쪽의 optimizer가 오른쪽의 optimizee 성능에 대한 정보를 받고 optimizee를 수정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Transfer learning and generalization\n",
    "\n",
    "이 논문에서 풀고하 하는 문제는 Meta learning의 관점에서 논의됩니다.\n",
    "\n",
    "Meta learning의 관점에서 generalization은 평소와는 약간 다른 의미를 가집니다.\n",
    "\n",
    "일반적인 기계학습에서 generalization은\n",
    "\n",
    "알고리즘이 학습된 적 없는 데이타에 대해서 어떻게 동작하는지에 관심이 있습니다.\n",
    "\n",
    "이 논문에서는\n",
    "\n",
    "1. 학습된 적 없는 데이터들이\n",
    "2. 알고리즘에서 사용할 데이터입니다.\n",
    "3. 그리고 이경우 generalization은\n",
    "4. transfer learning이 어떻게 되는지에 관심이 있습니다.\n",
    "5. ex) MNIST to CIFAR10\n",
    "\n",
    "#### Transfer Learning\n",
    "\n",
    "기존의 만들어진 모델을 사용하여 새로운 모델을 만들시 학습을 빠르게 하며, 예측을 더 높이는 방법입니다.\n",
    "\n",
    "실질적으로 Convolution network을 처음부터 학습시키는 일은 많지 않습니다. 대부분의 문제는 이미 학습된 모델을 사용해서 문제를 해결할 수 있습니다.\n",
    "\n",
    "복잡한 모델일수록 학습시키기 어렵습니다. 어떤 모델은 2주정도 걸릴수 있으며, 비싼 GPU 여러대를 사용하기도 합니다.\n",
    "\n",
    "layers의 갯수, activation, hyper parameters등등 고려해야 할 사항들이 많으며, 실질적으로 처음부터 학습시키려면 많은 시도가 필요합니다.\n",
    "\n",
    "결론적으로 이미 잘 훈련된 모델이 있고, 특히 해당 모델과 유사한 문제를 해결시 transfer learining을 사용합니다.\n",
    "\n",
    "1. 새로 훈련할 데이터가 적지만 original 데이터와 유사할 경우\n",
    "> 데이터의 양이 적어 fine-tune (전체 모델에 대해서 backpropagation을 진행하는 것) 은 over-fitting의 위험이 있기에 하지 않습니다. 새로 학습할 데이터는 original 데이터와 유사하기 때문에 이 경우 최종 linear classfier 레이어만 학습을 합니다.\n",
    "\n",
    "2. 새로 훈련할 데이터가 매우 많으며 original 데이터와 유사할 경우\n",
    "> 새로 학습할 데이터의 양이 많다는 것은 over-fitting의 위험이 낮다는 뜻이므로, 전체 레이어에 대해서 fine-tune을 합니다.\n",
    "\n",
    "3. 새로 훈련할 데이터가 적으며 original 데이터와 다른 경우\n",
    "> 데이터의 양이 적기 때문에 최종 단계의 linear classifier 레이어를 학습하는 것이 좋을 것입니다. 반면서 데이터가 서로 다르기 때문에 거의 마지막부분 (the top of the network)만 학습하는 것은 좋지 않습니다. 서로 상충이 되는데.. 이 경우에는 네트워크 초기 부분 어딘가 activation 이후에 특정 레이어를 학습시키는게 좋습니다.\n",
    "\n",
    "4. 새로 훈련할 데이터가 많지만 original 데이터와와 다른 경우\n",
    "> 데이터가 많기 때문에 아예 새로운 ConvNet을 만들수도 있지만, 실적적으로 transfer learning이 더 효율이 좋습니다. 전체 네트워크에 대해서 fine-tune을 해도 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 A brief history and related work\n",
    "\n",
    "```python\n",
    "def a_brief_history_and_related_work():\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Learning to learn with recurrent neural networks\n",
    "\n",
    "optimizer의 파라미터 $\\phi$를 통해 최종 optimizee의 파라미터 $\\theta$를 얻게 될 것입니다.\n",
    "\n",
    "최종 optimizee의 파라미터들은 $\\theta^*(f, \\phi))$로 표기합니다.\n",
    "\n",
    "우리는 이 파라미터 $\\theta^*$가 적용된 $f$들을 구하고,\n",
    "\n",
    "$f$들의 평균을 $\\phi$를 이용한 optimizer의 비용으로 생각 할 수 있습니다.\n",
    "\n",
    "> \"$f$들\"은 정확히 무엇인가요..? \n",
    "\n",
    "### $L(\\phi)=E_f\\left[ f(\\theta^*(f, \\phi)) \\right] $\n",
    "\n",
    "### - $\\theta^*(f, \\phi))$: final optimizee parameters\n",
    "### - $\\phi$: optimizer parameters\n",
    "### - $f$: function in question\n",
    "\n",
    "우리는 비용에 대해 일련의 업데이트 과정을 거쳐가야합니다.\n",
    "\n",
    "이 일련의 업데이트는 RNN구조가 적합해 보입니다. RNN 알고리즘을 $m$이라고 부르도록 하겠습니다.\n",
    "\n",
    "$m$의 아웃풋이 바로 $t$시점의 업데이트에 적용될 $g_t$가 됩니다.\n",
    "\n",
    "RNN 알고리즘 $m$은 파라피터 $\\phi$를 가지며, $h_t$를 입력으로 받아 상태가 결정되는 함수입니다.\n",
    "\n",
    "### $L(\\phi)=E_f\\left[ \\sum _{ t=1 }^{ T }{ w_tf(\\theta_t) } \\right]$\n",
    "### $\\qquad$ where\n",
    "### $\\qquad\\qquad \\theta_{t+1}=\\theta_t+g_t$\n",
    "### $\\qquad\\qquad \\left[ \\begin{matrix} g_t \\\\ h_{t+1} \\end{matrix} \\right] = m(\\nabla_t, h_t, \\phi)$\n",
    "\n",
    "\n",
    "비용함수가 위와 같이 바뀌었습니다.\n",
    "\n",
    "$\\nabla_t=\\nabla_{\\theta}f(\\theta_t)$와 같이 간단히 표기합니다.\n",
    "\n",
    "위 식에서 $t=T$일 경우만 $w_t=1$이며, 나머지경우는 $w_t=0$일 경우 처음 제시한 식과 같아집니다.\n",
    "\n",
    "이제 다 왔습니다.\n",
    "\n",
    "다음의 계산그래프를 이용해 $L(\\phi)$를 optimizer의 파라미터 $\\phi$를 학습해서 최소화하면 됩니다.\n",
    "\n",
    "![img](img/1.png)\n",
    "\n",
    "optimizer는 $\\partial L(\\phi)/\\partial\\phi$를 계산해 업데이트 됩니다.\n",
    "\n",
    "이 과정에서 만약 최종 $\\nabla_T$만을 사용한다면,\n",
    "\n",
    "RNN을 학습 시킬 때 효과적으로 gradient descent가 이루어지지 않습니다.\n",
    "\n",
    "따라서 모든$t$ 단계에 대해서 $f_t$를 구하고 이를 사용합니다.\n",
    "\n",
    "논문에서는 간단하게 모든 $w_t$를 1로 두었습니다.\n",
    "\n",
    "계산그래프에서는 실선을 통해서만 backpropagation을 수행합니다. (점선의 weight를 drop합니다.)\n",
    "\n",
    "이는 optimizee의 파라미터가 optimizer의 파라미터 $\\phi$에 영향 받지 않는다는 가정을 의미합니다.\n",
    "\n",
    "### $\\partial \\nabla_t/\\partial\\phi = 0$\n",
    "\n",
    "> 이는 $f$의 2계 도함수를 구할 필요가 없다는 것과 같은 말이랍니다..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Coordinatewise LSTM optimizer\n",
    "\n",
    "이 과정을 수행하기 위해서는, 적어도 10개의 수천개의 파라미터를 가진 알고리즘을 계산해야합니다.\n",
    "\n",
    "따라서, fully connected RNN을 이용하기에는 비용이 너무 큽니다.\n",
    "\n",
    "따라서 coordinatewise network architecture를 이용합니다.\n",
    "\n",
    "![img](img/2.png)\n",
    "\n",
    "사실 LSTM은 한개이며 LSTM의 파라미터를 공유합니다.\n",
    "\n",
    "optimizee의 각 파라미터들에 대해서, 공통된 optimizer의 파라미터를 이용합니다.\n",
    "\n",
    "($\\theta^n$이 하나의 배치와도 같다고 생각하기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Experiments\n",
    "\n",
    "![img](img/3.png)\n",
    "![img](img/4.png)\n",
    "![img](img/5.png)\n",
    "![img](img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료\n",
    "- [PR-031](https://www.youtube.com/watch?v=p55H46RiZ6k&t=0s&list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS&index=32)\n",
    "- [incredible.ai](http://incredible.ai/artificial-intelligence/2017/05/13/Transfer-Learning/)\n",
    "- [deepmind github](https://github.com/deepmind/learning-to-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "img[alt='img']{\n",
       "    display: inline-block;\n",
       "    max-width: 800px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    "img[alt='img']{\n",
    "    display: inline-block;\n",
    "    max-width: 800px;\n",
    "}\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
