{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 해킹\n",
    "\n",
    "## Adversarial Attacks?\n",
    "\n",
    "![](./img/1.png)\n",
    "\n",
    "> - 위는 원본이미지, 아래는 변형한 이미지\n",
    "> - 사람이 보기엔 여전히 구분 가능한데도 불구하고, 머신러닝 모델에서는 구분하지 못함.\n",
    "> - **과적합의 예시**\n",
    "\n",
    "## Why?\n",
    "\n",
    "> - 딥러닝 모델은 일반적으로 Bias error가 낮은 대신 Variance error가 높다.\n",
    "> - 데이터가 많으면 그에 맞춰서 학습이 잘되겠지만,\n",
    "> - 모델이 감안하지 못하는 조건에서 망가질 우려가 크다.\n",
    "\n",
    "#### Bias error\n",
    "> 편향은 학습 알고리즘에서 잘못된 가정을 했을 때 발생하는 오차이다. 높은 편향값은 알고리즘이 데이터의 특징과 결과물과의 적절한 관계를 놓치게 만드는 과소적합(underfitting) 문제를 발생 시킨다.\n",
    "\n",
    "#### Variance error\n",
    "> 분산은 트레이닝 셋에 내재된 작은 변동(fluctuation) 때문에 발생하는 오차이다. 높은 분산값은 큰 노이즈까지 모델링에 포함시켜버리는 과적합(overfitting) 문제를 발생 시킨다.\n",
    "\n",
    "![](./img/3.png)\n",
    "![](./img/4.png)\n",
    "\n",
    "## Adversarial Attacks!\n",
    "\n",
    "[Practical Black-Box Attacks against Machine Learning](https://arxiv.org/abs/1602.02697)\n",
    "\n",
    "![](./img/2.png)\n",
    "\n",
    "- 최소한의 변형으로 최대한 모델이 못맞추게 해버리자\n",
    "- 데이터를 조금 씩 바꿔보며 여러번 찔러보기 식의 공격\n",
    "- 테슬라가 작정하고 우버를 망하게 하려한다면...?\n",
    "\n",
    "#### Black-Box Attack (Transferability)\n",
    "\n",
    "> 공격 대상이 될 모델 A의 내부 구조를 몰라도\n",
    "\n",
    "> 공격 대상 모델과 비슷한 아웃풋을 내는 공격용 모델B를 만든다면\n",
    "\n",
    "> A와 B는 같은 **Adversarial Example**에 대해 공격당할 확률이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defense Against Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adversarial Attack은 딥러닝 발전에서 무시할수 없는 위협이다. 막아야한다.\n",
    "\n",
    "- 이 논문은 Defense계의 SOTA다.\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 역할의 목적\n",
    "\n",
    "#### Attacker\n",
    "- 의심받지 않고 (최소한의 시도로) 모델로부터 원하는 결과를 이끌어 내기\n",
    "\n",
    "#### Defender\n",
    "- 가능한 모든 기존의 공격과 가능한 모든 데이터에 대해서 대처하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defence의 난해함\n",
    "\n",
    "- 공격받은 적 없는 모델이 안전하다는 것을 어떻게 증명하는가.\n",
    "- 무슨 기준으로 안전한 정도를 측정하겠는가.\n",
    "\n",
    "## Adversarial Examples\n",
    "\n",
    "#### 공격에 쓰일 데이타의 특징\n",
    "1. 이상적인 데이타와 겉보기에 차이가 크지 않은 경우\n",
    "    - 최소한의 노력으로 공격 데이타를 만든다.\n",
    "    - 모델은 이걸 정상 데이타로 인식한다.\n",
    "    - 공격데이타를 아무 조치없이 모델이 학습하면 심각한 문제가 초래될 수 있다.\n",
    "2. 공격에 쓰일 데이타가 말도 안되게 생긴 경우\n",
    "    - 사람이 보기에는 얼토당토 않다.\n",
    "    - 모델이 보기에는 높은 확률로 특정 클래스이며, 심지어 자연스러운 데이타이다.\n",
    "    - 이런 공격을 보면, 대체 모델이 무었을 배웠는지 근본적인 당위성을 따질 필요가 생긴다.\n",
    "    - 어쩌면 딥러닝은 그냥 기억만 할뿐, 특징 추출을 안했는지도 모른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해명\n",
    "\n",
    "> 딥러닝이 이런 공격에 취약한 이유는 완벽히 해명되지 않았지만, \n",
    "\n",
    "> 취약한 이유에 대한 다양한 이론이 제시되고 있다.\n",
    "\n",
    "#### 이론 1\n",
    "\n",
    "1. 딥러닝의 높은 복잡도와 비선형성이\n",
    "2. Train 데이타가 탐험하지 못한 영역에\n",
    "3. 랜덤한 레이블을 달아버렸을 수도 있다.\n",
    "4. 이런 문제의 영역은 아주 작은 부분일 것이다.\n",
    "5. 따라서 공격데이타가 존재할 확률도 적다.\n",
    "6. 하지만 Transferability를 해명하지 못함\n",
    "    > 공격 대상이 될 모델 A의 내부 구조를 몰라도\n",
    "    > 공격 대상 모델과 비슷한 아웃풋을 내는 공격용 모델B를 만든다면\n",
    "    > A와 B는 같은 **Adversarial Example**에 대해 공격당할 확률이 높다.\n",
    "    \n",
    "#### 이론 2\n",
    "\n",
    "- Linearity Hypothesis (위와 같은 공격은 Linear모델에서도 일어나는 현상이다.)\n",
    "- 딥러닝은 비선형 결정 경계면을 그린다.\n",
    "- 그러나 사실 인풋 데이타 입장에서 보면 결정경계면은 거의 선형처럼 보일 수 있다.\n",
    "- 그 선형에 직교하는 방향으로 데이타를 변경하면 쉽게 Adversarial Example을 구할 수 있다.\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이론 3\n",
    "\n",
    "- 신경망이 깊을수록 공격 요소가 증폭되는 효과가 있을 수 있다.\n",
    "- 레이어를 따라 전파되면서 오류가 커지는 것으로 이해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Defense Method\n",
    "\n",
    "- 이 논문에서는 two-fold defense method를 제안한다.\n",
    "\n",
    "- 이는 거의 비용이 없고 쉬운 방어법이다.\n",
    "\n",
    "- 아이디어의 핵심은 딥러닝의 공통된 약점을 보강하며,\n",
    "\n",
    "- 결정경계를 더 부드럽게 만드는 것이다.\n",
    "\n",
    "- 어떤 공격이든 상관없이 범용적으로 이용 가능하다.\n",
    "\n",
    "다시 이 논문에서 제시한 방어법의 성능을 보자. 짱이지?\n",
    "\n",
    "![](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Defenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary 정의\n",
    "\n",
    "논문이 제시하는 공격에 대한 정의부터 먼저 이해해보자.\n",
    "\n",
    "공격자는 보통 다음 두가지 기준으로 분류될 수 있다.\n",
    "\n",
    "- 공격 목적 (악의의 정도 또는 공격으로 얻는 이득의 정도)\n",
    "\n",
    "- 공격 가능 범위\n",
    "\n",
    "***이 논문***에서는 공격 목적은 신경 안쓰도록 하겠다.\n",
    "\n",
    "#### 가능 범위\n",
    "\n",
    "***이 논문***에서는 공격자는 Test Data에 접근 가능하며,\n",
    "\n",
    "경우에 따라서, 훈련된 모델에도 접근 할 수 있다.\n",
    "\n",
    "#### 불가능 범위\n",
    "\n",
    "Training Data에는 접근 불가능하다.\n",
    "\n",
    "당연히 훈련 과정도 접근 불가능하다.\n",
    "\n",
    "#### 공격자의 지식\n",
    "\n",
    "공격자의 지식은 모델이 돌아가는 시스템만 아는 것 부터 모델의 세부 파라미터 값을 아는 것 까지 다양한 경우가 존재할 수 있다.\n",
    "\n",
    "#### \"Whitebox model\" VS \"Blackbox model\"\n",
    "\n",
    "당연히 공격자는 Whitebox모델에서 더 잘 공격한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounded RELU\n",
    "\n",
    "방어법 1장\n",
    "![](./img/7.png)\n",
    "\n",
    "> 그냥 렐루 위에도 바운드 친 것.\n",
    "\n",
    "그냥 렐루는 인풋데이타의 작은 변화를 마지막 아웃풋까지 전파하면서 거대하게 증폭시키는 특징이 있다.\n",
    "\n",
    "이를 방지하기 위해 다음처럼 바꾸자.\n",
    "\n",
    "![](./img/10.png)\n",
    "\n",
    "이때 t는 인풋의 크기에 따라 잘 조정해 주도록 한다.\n",
    "\n",
    "포화상태(Saturate)인 t를 잘 구해줘야하며, t가 너무 크면 당연히 효과가 없어진다.\n",
    "\n",
    "- 그냥 렐루는 인풋값의 변화의 최대 증폭이 다음 식으로 표현된다.\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "- 수정된 렐루는 인풋값의 변화의 최대 증폭이 다음 식으로 표현된다.\n",
    "\n",
    "![](./img/8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Data Augmentation\n",
    "\n",
    "방어법 2장\n",
    "\n",
    "![](./img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial training\n",
    "\n",
    "- Data Augmentation의 한 종류라고 생각하면 편하다.\n",
    "\n",
    "- 진짜로 얻어진 데이터와 \"진짜에서 변형된 데이타\"를 서로 같은 레이블에 두고 학습한다.\n",
    "\n",
    "- 모델이 좀더 일반화 된다고 볼 수 있다.\n",
    "\n",
    "#### 하지만\n",
    "\n",
    "- Whitebox 공격에 대해서만 유효할 뿐 Blackbox공격은 방어하지 못한다.\n",
    "\n",
    "#### 왜냐하면\n",
    "\n",
    "- 이런 방식으로는 모델이 가능한 모든 벡터 방향으로 방어하는게 아니기 때문이다.\n",
    "- 사실 왠만해서는 방어 안되는 방향이 더 많다. (인풋이 아주 고차원이니까)\n",
    "- 따라서 다른 방향으로 공격하면 속수무책으로 당한다.\n",
    "\n",
    "#### 게다가\n",
    "\n",
    "- 주어진 데이타가 닿지 않는 불분명한 구역에 대해 명확한 결정을 내릴 메카니즘이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Data Augmentation\n",
    "\n",
    "- 가우시안 노이즈를 이용해서 데이타를 Augmentation하자\n",
    "\n",
    "- 장점1: 다양한 방향으로 학습 가능\n",
    "\n",
    "- 장점2: 모델의 확신을 Smoothing함\n",
    "\n",
    "- 두개의 장점 다 가져가려면 Gaussian Noise 쓰자.\n",
    "\n",
    "![](./img/16.png)\n",
    "![](./img/15.png)\n",
    "![](./img/14.png)\n",
    "\n",
    "#### Toy Dataset\n",
    "\n",
    "![](./img/13.png)\n",
    "![](./img/12.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "## Metric\n",
    "\n",
    "#### 데이터 변화 정도에 따라 어디까지 방어 가능한지\n",
    "![](./img/20.png)\n",
    "\n",
    "#### 데이터 변화 정도에 따라 로스가 얼마나 강건하게 변하는지\n",
    "![](./img/19.png)\n",
    "\n",
    "## Table\n",
    "\n",
    "![](./img/18.png)\n",
    "![](./img/17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
