{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW DOES BATCH NORMALIZATION HELP OPTIMIZATION?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN (Batch Normalization)\n",
    "\n",
    "- BN은 모델에서 Layer에 추가 Normalization Layer를 하나 더 붙여\n",
    "\n",
    "- Layer의 인풋의 분포를 안정화시킨다고 알려져있다.\n",
    "\n",
    "- 이로 인해 BN은 DNN에서\n",
    "    - 더 빠른 학습을 돕는다\n",
    "    - 더 안정적인 학습을 돕는다\n",
    "\n",
    "- 라고 여겨질뿐... 사실 왜 이게 좋은지 명확히 알려지진 않았다.\n",
    "- BN을 대체하는 기법도 많이 나왔는데, 이해도가 부족해서 많이 사용되지 않고있다.\n",
    "\n",
    "## BN의 장점에 대한 믿음\n",
    "\n",
    "- BN이 Layer 인풋의 분포를 조정하게 되는데\n",
    "- 이는 ICS(Internal Covariate Shift)를 줄여주는 효과가 있다.\n",
    "- ICS는 Training과정에 부정적인 영향을 끼친다\n",
    "\n",
    "## ICS(Internal Covariate Shift)\n",
    "\n",
    "### 비유\n",
    "![](./img/1.png)\n",
    "![](./img/2.png)\n",
    "- 아래 사진에서 c와 d같은 제한장치가 필요하며 그것이 BN\n",
    "\n",
    "### 원인\n",
    "\n",
    "-  학습 시점 t에서 n번째 Layer는 n-1번째 Layer의 Output을 Input으로 받는다.\n",
    "\n",
    "-  학습 시점 t+1에서 n-1번째 Layer의 파라미터가 업데이트 되었다.\n",
    "\n",
    "-  이로 인해 학습 시점 t+1에서 n번째 Layer가 받는 Input값의 분포도 변화하게 되었다.\n",
    "\n",
    "-  이를 ICS라고 부르며 학습을 방해하는 요소로 여겨진다.\n",
    "\n",
    "## 그러나 이 논문에서는\n",
    "\n",
    "> 그래서 너희는 ICS가 Training에 어떻게 나쁜 영향을 주는지 제대로 알고 있니?\n",
    "\n",
    "> ICS로 BN의 장점을 설명하기엔 증거가 너무 부족한거 아니니?\n",
    "\n",
    "> 사실 내가 연구해보니까 ICS를 줄이는 일은 BN의 핵심 장점이 아니야.\n",
    "\n",
    "> 사실 ICS는 학습 퍼포먼스랑 별로 상관도 없어\n",
    "\n",
    "> 심지어.. 어쩌면 BN은 ICS를 줄이지도 못할수도 있어\n",
    "\n",
    "> BN에 대해 우리가 연구한 내용을 들어봐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH NORMALIZATION AND INTERNAL COVATIATE SHIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "- Basic VGG Net 모델을 CIFAR-10에 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "![](./img/3.png)\n",
    "\n",
    "- 요약: 배치놈을 적용하니 더 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/4.png)\n",
    "\n",
    "- 요약: 근데 ICS는 그닥 안줄어든거같은데..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 합당한 질문\n",
    "\n",
    "1. BN이 ICS를 줄이긴 해?\n",
    "\n",
    "2. ICS를 줄이는게 학습에 좋은거긴 해?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BN의 당위성을 검증하기 위한 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BN이 있는 레이어에 새로운 레이어를 하나 추가하도록 한다\n",
    "- 새로운 레이어는 BN 뒤에 오며, 인풋에 Random Noise를 추가한다\n",
    "- Random Noise는 평균이 0도 아니고 분산이 Unit도 아니다.\n",
    "- 심지어 매 학습 시점마다 평균과 분산이 바뀐다.\n",
    "- **이를 Noisy Batchnorm이라고 명명한다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/5.png)\n",
    "![](./img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICS가 Training능력에 중요한 요소인가? & NB은 ICS를 줄이는가?\n",
    "\n",
    "![](./img/8.png)\n",
    "![](./img/7.png)\n",
    "\n",
    "- #### BN은 어떤 경우 심지어 ICS를 늘린다\n",
    "- #### DLN은 Deep Linear Network를 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY DOES BATCHNORM WORK?\n",
    "\n",
    "## BN의 Origin 논문에 의하면\n",
    "\n",
    "- BN은 ICS말고도\n",
    "- Exploding/Vanishing Gradient 문제를 해결해주며,\n",
    "- 모델을 하이퍼파리미터나 파라미터의 변동에 강건하게 해준다고 한다.\n",
    "- 그러나 이건 너무 당연한 말이다. BN의 특출난 성능을 설명해주지 못한다.\n",
    "- BN말고도 이런걸 해결하는 방식은 많다. 그럼에도 불구하고 BN은 강력하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 우리가 발견한 BN의 장점은\n",
    "\n",
    "> it reparametrizes the underlying optimization problem to make its landscape significantly more smooth.\n",
    "\n",
    "- *립시츠* 조건의 관점에서 목적함수의 최적화 과정을 스무딩한다.\n",
    "\n",
    "![](./img/9.png)\n",
    "\n",
    "#### 립시츠 함수\n",
    "\n",
    "> 해석학에서, 립시츠 연속 함수(영어: Lipschitz-continuous function)는 두 점 사이의 거리를 일정 비 이상으로 증가시키지 않는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BN은 레이어의 모수들을 재모수화(reparametrization) 시키면서 립시츠 스무딩을 실현한다.\n",
    "- 애초에 우리가 최적화 할 목적함수는 사실상 Convex도 아니고 무진장 불안정하다\n",
    "- SGD과정에서 스무딩이 안되면 당연히 학습 과정이 불안정할 수 밖에 없다.\n",
    "    - 갑자기 로스값이 크게 변할 수 있다. (Exploding)\n",
    "    - 평평한 구간을 만나 학습이 정지될 수도 있다. (Vanishing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the Optimization Landscape\n",
    "\n",
    "![](./img/11.png)\n",
    "![](./img/10.png)\n",
    "\n",
    "- (b) 각 스탭별 립쉬츠 관점에서 최대 L\n",
    "- (c) 각 스탭별 각각의 변수 노드에 적용되는 Gradient 방향의 L2 Distance의 분산 (부정확함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/20.png)\n",
    "![](./img/21.png)\n",
    "![](./img/22.png)\n",
    "![](./img/23.png)\n",
    "![](./img/24.png)\n",
    "![](./img/25.png)\n",
    "![](./img/26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
